---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-job-config
  namespace: ml-otel-demo
data:
  JOB_ID: "k8s-training-job-001"
  MODEL_NAME: "resnet-50"
  DATASET: "imagenet"
  METRICS_FILE_PATH: "/shared/metrics/current.json"
  WRITE_INTERVAL: "10"
  TOTAL_EPOCHS: "5"
  BATCHES_PER_EPOCH: "50"

---

apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
  namespace: ml-otel-demo
  labels:
    app: ml-training
spec:
  # Job will run once and complete
  completions: 1
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: ml-training
        job-name: ml-training
    spec:
      restartPolicy: OnFailure

      # Shared volume for metrics
      volumes:
        - name: metrics-volume
          emptyDir: {}

      containers:
        # Main ML training container
        - name: ml-job
          image: ghcr.io/openteams-ai/mock-ml-job:latest
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: ml-job-config
          volumeMounts:
            - name: metrics-volume
              mountPath: /shared/metrics
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
        # OpenTelemetry Metrics Bridge Sidecar
        - name: otel-metrics-bridge
          image: ml-metrics-bridge:latest
          imagePullPolicy: IfNotPresent
          env:
            # OpenTelemetry configuration
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "otel-collector:4317"
            - name: OTEL_SERVICE_NAME
              value: "ml-metrics-bridge"
            
            # Metrics file configuration
            - name: METRICS_FILE_PATH
              value: "/shared/metrics/current.json"
            
            # Collection interval
            - name: COLLECTION_INTERVAL
              value: "10"
            
            # Logging level
            - name: LOG_LEVEL
              value: "INFO"
          volumeMounts:
            - name: metrics-volume
              mountPath: /shared/metrics
              readOnly: true
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          # Sidecar should continue running even if ML job completes
          # This ensures final metrics are exported
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "sleep 15"]